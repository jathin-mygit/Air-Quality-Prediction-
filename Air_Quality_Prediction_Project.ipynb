{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Air Quality Prediction Using Machine Learning\n",
        "\n",
        "**Project by:** INLIGHN TECH  \n",
        "**Platform:** Google Colab  \n",
        "**Difficulty:** Medium  \n",
        "**Technologies:** Python, scikit-learn, pandas, matplotlib, seaborn\n",
        "\n",
        "---\n",
        "\n",
        "## Project Description\n",
        "This project focuses on predicting air quality using machine learning techniques. We will build a model to analyze and forecast air pollution levels based on historical data.\n",
        "\n",
        "**Dataset Attributes:**\n",
        "- `city`: City name\n",
        "- `date`: Date of measurement\n",
        "- `aqi`: Air Quality Index (Target variable)\n",
        "- `co`: Carbon Monoxide (mg/m³)\n",
        "- `no`: Nitric Oxide (µg/m³)\n",
        "- `no2`: Nitrogen Dioxide (µg/m³)\n",
        "- `o3`: Ozone (µg/m³)\n",
        "- `so2`: Sulfur Dioxide (µg/m³)\n",
        "- `pm2_5`: PM2.5 particles (µg/m³)\n",
        "- `pm10`: PM10 particles (µg/m³)\n",
        "- `nh3`: Ammonia (µg/m³)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Import Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import all required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime, timedelta\n",
        "import random\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Machine Learning libraries\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "# Set style for visualizations\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"✅ All libraries imported successfully!\")\n",
        "print(\"📊 Visualization style configured\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Data Generation (For Demonstration)\n",
        "\n",
        "*Note: In a real project, you would load data from the UCI ML Repository or your own dataset file.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate realistic air quality dataset for demonstration\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "print(\"🏗️ CREATING AIR QUALITY DATASET\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "# Define major Indian cities\n",
        "cities = ['Delhi', 'Mumbai', 'Kolkata', 'Chennai', 'Bangalore', 'Hyderabad', \n",
        "          'Ahmedabad', 'Pune', 'Surat', 'Jaipur', 'Lucknow', 'Kanpur']\n",
        "\n",
        "# Generate date range (2 years of data)\n",
        "start_date = datetime(2022, 1, 1)\n",
        "end_date = datetime(2023, 12, 31)\n",
        "date_range = pd.date_range(start=start_date, end=end_date, freq='D')\n",
        "\n",
        "print(f\"📊 Generating dataset for {len(cities)} cities over {len(date_range)} days\")\n",
        "\n",
        "# Create realistic air quality data\n",
        "data = []\n",
        "\n",
        "for city in cities:\n",
        "    for date in date_range:\n",
        "        # Add seasonal and city-specific variations\n",
        "        seasonal_factor = 1 + 0.3 * np.sin(2 * np.pi * date.timetuple().tm_yday / 365)\n",
        "        \n",
        "        # City pollution factors\n",
        "        city_factors = {\n",
        "            'Delhi': 1.8, 'Kanpur': 1.6, 'Lucknow': 1.4, 'Ahmedabad': 1.3,\n",
        "            'Mumbai': 1.2, 'Kolkata': 1.3, 'Pune': 1.1, 'Jaipur': 1.2,\n",
        "            'Chennai': 1.0, 'Bangalore': 0.9, 'Hyderabad': 1.0, 'Surat': 1.1\n",
        "        }\n",
        "        \n",
        "        city_factor = city_factors.get(city, 1.0)\n",
        "        combined_factor = seasonal_factor * city_factor\n",
        "        \n",
        "        # Generate correlated pollutant values\n",
        "        row = {\n",
        "            'city': city,\n",
        "            'date': date.strftime('%Y-%m-%d'),\n",
        "            'aqi': max(10, min(300, int(np.random.normal(80, 40) * combined_factor))),\n",
        "            'co': round(max(0.1, np.random.exponential(2) * combined_factor), 2),\n",
        "            'no': max(1, int(np.random.gamma(2, 15) * combined_factor)),\n",
        "            'no2': max(5, int(np.random.gamma(2, 20) * combined_factor)),\n",
        "            'o3': max(10, int(np.random.gamma(2, 25) * combined_factor)),\n",
        "            'so2': max(1, int(np.random.gamma(2, 10) * combined_factor)),\n",
        "            'pm2_5': max(5, int(np.random.gamma(2, 18) * combined_factor)),\n",
        "            'pm10': max(10, int(np.random.gamma(2, 30) * combined_factor)),\n",
        "            'nh3': max(1, int(np.random.gamma(2, 8) * combined_factor))\n",
        "        }\n",
        "        \n",
        "        # Apply realistic limits\n",
        "        row['aqi'] = max(10, min(300, row['aqi']))\n",
        "        row['co'] = max(0.1, min(15.0, row['co']))\n",
        "        row['no'] = max(1, min(200, row['no']))\n",
        "        row['no2'] = max(5, min(150, row['no2']))\n",
        "        row['o3'] = max(10, min(180, row['o3']))\n",
        "        row['so2'] = max(1, min(100, row['so2']))\n",
        "        row['pm2_5'] = max(5, min(150, row['pm2_5']))\n",
        "        row['pm10'] = max(10, min(250, row['pm10']))\n",
        "        row['nh3'] = max(1, min(50, row['nh3']))\n",
        "        \n",
        "        data.append(row)\n",
        "\n",
        "# Create DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Add some missing values (-200) as mentioned in requirements\n",
        "missing_indices = np.random.choice(df.index, size=int(0.02 * len(df)), replace=False)\n",
        "missing_columns = ['co', 'no2', 'pm2_5', 'pm10']\n",
        "\n",
        "for idx in missing_indices:\n",
        "    col = np.random.choice(missing_columns)\n",
        "    df.at[idx, col] = -200\n",
        "\n",
        "print(f\"✅ Dataset created successfully!\")\n",
        "print(f\"📊 Dataset shape: {df.shape}\")\n",
        "print(f\"🏙️ Cities: {len(df['city'].unique())} cities\")\n",
        "print(f\"📅 Date range: {df['date'].min()} to {df['date'].max()}\")\n",
        "print(f\"❓ Missing values (-200): {(df == -200).sum().sum()} entries\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Data Understanding and Exploration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display basic information about the dataset\n",
        "print(\"=== DATASET OVERVIEW ===\")\n",
        "print(f\"Dataset shape: {df.shape}\")\n",
        "print(f\"\\nColumn names: {list(df.columns)}\")\n",
        "print(f\"\\nData types:\")\n",
        "print(df.dtypes)\n",
        "\n",
        "print(\"\\n📋 First 10 rows:\")\n",
        "display(df.head(10))\n",
        "\n",
        "print(\"\\n📊 Statistical Summary:\")\n",
        "display(df.describe())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check for missing values\n",
        "print(\"=== MISSING VALUE ANALYSIS ===\")\n",
        "\n",
        "# Standard missing values (NaN)\n",
        "print(\"\\n🔍 Standard missing values (NaN):\")\n",
        "nan_missing = df.isnull().sum()\n",
        "print(nan_missing[nan_missing > 0])\n",
        "\n",
        "# Missing values marked as -200\n",
        "print(\"\\n❓ Missing values marked as -200:\")\n",
        "missing_200 = (df == -200).sum()\n",
        "print(missing_200[missing_200 > 0])\n",
        "\n",
        "# Cities and date range analysis\n",
        "print(\"\\n🏙️ Cities in dataset:\")\n",
        "print(f\"   {', '.join(sorted(df['city'].unique()))}\")\n",
        "print(f\"\\n📅 Date range: {df['date'].min()} to {df['date'].max()}\")\n",
        "\n",
        "# Target variable analysis\n",
        "print(\"\\n🎯 Target Variable (AQI) Analysis:\")\n",
        "aqi_stats = df['aqi'].describe()\n",
        "print(aqi_stats)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Data Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create visualizations for data exploration\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "axes = axes.ravel()\n",
        "\n",
        "# Plot distributions of key pollutants\n",
        "pollutants = ['aqi', 'co', 'no2', 'pm2_5', 'pm10', 'o3']\n",
        "\n",
        "for i, pollutant in enumerate(pollutants):\n",
        "    # Filter out -200 values for visualization\n",
        "    clean_data = df[df[pollutant] != -200][pollutant]\n",
        "    \n",
        "    axes[i].hist(clean_data, bins=30, alpha=0.7, edgecolor='black')\n",
        "    axes[i].set_title(f'{pollutant.upper()} Distribution')\n",
        "    axes[i].set_xlabel(pollutant)\n",
        "    axes[i].set_ylabel('Frequency')\n",
        "    axes[i].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.suptitle('Air Quality Parameters Distribution', y=1.02, fontsize=16)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Missing values heatmap\n",
        "plt.figure(figsize=(12, 8))\n",
        "missing_data = (df == -200).astype(int)\n",
        "sns.heatmap(missing_data.T, cbar=True, cmap='Reds', \n",
        "            xticklabels=False, yticklabels=df.columns)\n",
        "plt.title('Missing Values Heatmap (Red = Missing)', fontsize=14)\n",
        "plt.xlabel('Sample Index')\n",
        "plt.ylabel('Features')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data preprocessing pipeline\n",
        "print(\"=== DATA PREPROCESSING ===\")\n",
        "\n",
        "# Create a copy for processing\n",
        "df_processed = df.copy()\n",
        "\n",
        "# Step 1: Handle missing values (-200 → NaN → mean imputation)\n",
        "print(\"\\n🔧 Handling missing values...\")\n",
        "df_processed = df_processed.replace(-200, np.nan)\n",
        "\n",
        "# Fill missing values with column mean for numeric columns\n",
        "numeric_columns = df_processed.select_dtypes(include=[np.number]).columns\n",
        "for col in numeric_columns:\n",
        "    if df_processed[col].isnull().sum() > 0:\n",
        "        mean_value = df_processed[col].mean()\n",
        "        df_processed[col].fillna(mean_value, inplace=True)\n",
        "        print(f\"   ✅ Filled {col} missing values with mean: {mean_value:.2f}\")\n",
        "\n",
        "# Step 2: Convert date column to datetime\n",
        "print(\"\\n📅 Converting date column...\")\n",
        "df_processed['date'] = pd.to_datetime(df_processed['date'])\n",
        "print(\"   ✅ Date column converted to datetime64[ns]\")\n",
        "\n",
        "print(\"\\n✅ Data preprocessing completed!\")\n",
        "print(f\"📊 Final dataset shape: {df_processed.shape}\")\n",
        "print(f\"📊 Remaining null values: {df_processed.isnull().sum().sum()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Feature Engineering and Correlation Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature engineering and correlation analysis\n",
        "print(\"=== FEATURE ENGINEERING ===\")\n",
        "\n",
        "# Define features and target\n",
        "feature_columns = ['co', 'no', 'no2', 'o3', 'so2', 'pm2_5', 'pm10', 'nh3']\n",
        "target_column = 'aqi'\n",
        "\n",
        "print(f\"🎯 Target variable: {target_column}\")\n",
        "print(f\"🔢 Feature variables: {feature_columns}\")\n",
        "\n",
        "# Extract features and target\n",
        "X = df_processed[feature_columns]\n",
        "y = df_processed[target_column]\n",
        "\n",
        "print(f\"\\n📊 Features shape: {X.shape}\")\n",
        "print(f\"📊 Target shape: {y.shape}\")\n",
        "\n",
        "# Correlation analysis\n",
        "print(\"\\n🔗 Correlation with target variable (AQI):\")\n",
        "correlations = df_processed[feature_columns + [target_column]].corr()[target_column].sort_values(ascending=False)\n",
        "for feature, corr in correlations.items():\n",
        "    if feature != target_column:\n",
        "        print(f\"   {feature}: {corr:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Correlation heatmap\n",
        "plt.figure(figsize=(10, 8))\n",
        "correlation_matrix = df_processed[feature_columns + [target_column]].corr()\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0,\n",
        "            square=True, fmt='.3f')\n",
        "plt.title('Feature Correlation Matrix', fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Data Scaling and Train-Test Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature scaling and train-test split\n",
        "print(\"=== DATA SCALING AND SPLITTING ===\")\n",
        "\n",
        "# Scale features using StandardScaler\n",
        "print(\"⚖️ Scaling features using StandardScaler...\")\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "print(\"   ✅ Features scaled successfully\")\n",
        "\n",
        "# Split data into train and test sets\n",
        "print(\"\\n📊 Splitting data into train and test sets...\")\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_scaled, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "print(f\"   ✅ Training set: {X_train.shape[0]} samples\")\n",
        "print(f\"   ✅ Testing set: {X_test.shape[0]} samples\")\n",
        "print(f\"   ✅ Features: {X_train.shape[1]} dimensions\")\n",
        "\n",
        "print(\"\\n✅ Data preparation completed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 8: Model Training and Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define model evaluation function\n",
        "def evaluate_model(y_true, y_pred, model_name):\n",
        "    \"\"\"Calculate and display model evaluation metrics\"\"\"\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "    \n",
        "    print(f\"\\n📊 {model_name} Results:\")\n",
        "    print(f\"   Mean Absolute Error (MAE): {mae:.4f}\")\n",
        "    print(f\"   Root Mean Square Error (RMSE): {rmse:.4f}\")\n",
        "    print(f\"   R-squared (R²): {r2:.4f}\")\n",
        "    \n",
        "    return {'MAE': mae, 'RMSE': rmse, 'R2': r2}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train and evaluate multiple models\n",
        "print(\"🤖 TRAINING MULTIPLE MODELS\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "models = {}\n",
        "results = {}\n",
        "predictions = {}\n",
        "\n",
        "# 1. Random Forest Regressor\n",
        "print(\"\\n🌲 Training Random Forest Regressor...\")\n",
        "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "rf_pred = rf_model.predict(X_test)\n",
        "\n",
        "models['Random Forest'] = rf_model\n",
        "predictions['Random Forest'] = rf_pred\n",
        "results['Random Forest'] = evaluate_model(y_test, rf_pred, 'Random Forest')\n",
        "\n",
        "# 2. Linear Regression\n",
        "print(\"\\n📈 Training Linear Regression...\")\n",
        "lr_model = LinearRegression()\n",
        "lr_model.fit(X_train, y_train)\n",
        "lr_pred = lr_model.predict(X_test)\n",
        "\n",
        "models['Linear Regression'] = lr_model\n",
        "predictions['Linear Regression'] = lr_pred\n",
        "results['Linear Regression'] = evaluate_model(y_test, lr_pred, 'Linear Regression')\n",
        "\n",
        "# 3. Support Vector Regressor\n",
        "print(\"\\n🎯 Training Support Vector Regressor...\")\n",
        "svr_model = SVR(kernel='rbf', C=1.0, gamma='scale')\n",
        "svr_model.fit(X_train, y_train)\n",
        "svr_pred = svr_model.predict(X_test)\n",
        "\n",
        "models['SVR'] = svr_model\n",
        "predictions['SVR'] = svr_pred\n",
        "results['SVR'] = evaluate_model(y_test, svr_pred, 'SVR')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model comparison\n",
        "print(\"\\n📊 MODEL COMPARISON RESULTS\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Create comparison DataFrame\n",
        "comparison_df = pd.DataFrame(results).T\n",
        "comparison_df = comparison_df.round(4)\n",
        "display(comparison_df)\n",
        "\n",
        "# Find best model\n",
        "best_model_name = comparison_df['R2'].idxmax()\n",
        "best_r2_score = comparison_df.loc[best_model_name, 'R2']\n",
        "\n",
        "print(f\"\\n🏆 Best Model: {best_model_name}\")\n",
        "print(f\"   R² Score: {best_r2_score:.4f}\")\n",
        "print(f\"   MAE: {comparison_df.loc[best_model_name, 'MAE']:.4f}\")\n",
        "print(f\"   RMSE: {comparison_df.loc[best_model_name, 'RMSE']:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 9: Feature Importance Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature importance analysis (Random Forest)\n",
        "print(\"🌟 FEATURE IMPORTANCE ANALYSIS\")\n",
        "print(\"=\" * 35)\n",
        "\n",
        "# Get feature importance from Random Forest\n",
        "feature_importance = pd.DataFrame({\n",
        "    'feature': feature_columns,\n",
        "    'importance': rf_model.feature_importances_\n",
        "}).sort_values('importance', ascending=False)\n",
        "\n",
        "print(\"\\nRandom Forest Feature Importance:\")\n",
        "for _, row in feature_importance.iterrows():\n",
        "    print(f\"   {row['feature']}: {row['importance']:.3f}\")\n",
        "\n",
        "# Plot feature importance\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(data=feature_importance, x='importance', y='feature', palette='viridis')\n",
        "plt.title('Random Forest Feature Importance', fontsize=14)\n",
        "plt.xlabel('Importance Score')\n",
        "plt.ylabel('Features')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 10: Model Performance Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model performance comparison visualization\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "# Plot 1: Model R² comparison\n",
        "models_list = list(results.keys())\n",
        "r2_scores = [results[model]['R2'] for model in models_list]\n",
        "\n",
        "bars = axes[0].bar(models_list, r2_scores, color=['skyblue', 'lightgreen', 'coral'])\n",
        "axes[0].set_title('Model Performance Comparison (R² Score)', fontsize=14)\n",
        "axes[0].set_ylabel('R² Score')\n",
        "axes[0].set_ylim(0, max(r2_scores) * 1.1)\n",
        "\n",
        "# Add value labels on bars\n",
        "for bar, score in zip(bars, r2_scores):\n",
        "    axes[0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005,\n",
        "                f'{score:.3f}', ha='center', va='bottom')\n",
        "\n",
        "# Plot 2: Predicted vs Actual (best model)\n",
        "best_pred = predictions[best_model_name]\n",
        "axes[1].scatter(y_test, best_pred, alpha=0.6, color='blue')\n",
        "\n",
        "# Perfect prediction line\n",
        "min_val = min(y_test.min(), best_pred.min())\n",
        "max_val = max(y_test.max(), best_pred.max())\n",
        "axes[1].plot([min_val, max_val], [min_val, max_val], 'r--', lw=2, label='Perfect Prediction')\n",
        "\n",
        "axes[1].set_xlabel('Actual AQI')\n",
        "axes[1].set_ylabel('Predicted AQI')\n",
        "axes[1].set_title(f'{best_model_name} - Predicted vs Actual', fontsize=14)\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 11: Residual Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Residual analysis for the best model\n",
        "print(f\"📊 RESIDUAL ANALYSIS - {best_model_name}\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "best_pred = predictions[best_model_name]\n",
        "residuals = y_test - best_pred\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "# Residual plot\n",
        "axes[0].scatter(best_pred, residuals, alpha=0.6)\n",
        "axes[0].axhline(y=0, color='red', linestyle='--')\n",
        "axes[0].set_xlabel('Predicted AQI')\n",
        "axes[0].set_ylabel('Residuals')\n",
        "axes[0].set_title('Residual Plot', fontsize=14)\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Residual distribution\n",
        "axes[1].hist(residuals, bins=30, alpha=0.7, edgecolor='black')\n",
        "axes[1].axvline(residuals.mean(), color='red', linestyle='--', \n",
        "               label=f'Mean: {residuals.mean():.2f}')\n",
        "axes[1].set_xlabel('Residuals')\n",
        "axes[1].set_ylabel('Frequency')\n",
        "axes[1].set_title('Residual Distribution', fontsize=14)\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Residual statistics\n",
        "print(f\"\\nResidual Statistics:\")\n",
        "print(f\"   Mean: {residuals.mean():.4f}\")\n",
        "print(f\"   Std Dev: {residuals.std():.4f}\")\n",
        "print(f\"   Min: {residuals.min():.4f}\")\n",
        "print(f\"   Max: {residuals.max():.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 12: Save Results and Export Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save results and create output files\n",
        "print(\"💾 SAVING RESULTS\")\n",
        "print(\"=\" * 20)\n",
        "\n",
        "# Save processed dataset\n",
        "df_processed.to_csv('processed_air_quality_data.csv', index=False)\n",
        "print(\"✅ Processed dataset saved: 'processed_air_quality_data.csv'\")\n",
        "\n",
        "# Save model comparison results\n",
        "comparison_df.to_csv('model_comparison_results.csv')\n",
        "print(\"✅ Model comparison saved: 'model_comparison_results.csv'\")\n",
        "\n",
        "# Save feature importance\n",
        "feature_importance.to_csv('feature_importance.csv', index=False)\n",
        "print(\"✅ Feature importance saved: 'feature_importance.csv'\")\n",
        "\n",
        "# Save predictions\n",
        "predictions_df = pd.DataFrame({\n",
        "    'actual_aqi': y_test.values,\n",
        "    'predicted_aqi': best_pred,\n",
        "    'residual': residuals\n",
        "})\n",
        "predictions_df.to_csv('aqi_predictions.csv', index=False)\n",
        "print(\"✅ Predictions saved: 'aqi_predictions.csv'\")\n",
        "\n",
        "print(\"\\n📊 All results saved successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 13: Project Summary and Conclusions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Final project summary\n",
        "print(\"🎉 PROJECT SUMMARY\")\n",
        "print(\"=\" * 30)\n",
        "\n",
        "print(f\"📂 Dataset: Air Quality dataset with {len(df_processed):,} samples\")\n",
        "print(f\"🏙️ Cities: {len(df_processed['city'].unique())} cities\")\n",
        "print(f\"📅 Time Period: 2022-2023 (2 years)\")\n",
        "print(f\"🎯 Target: Air Quality Index (AQI)\")\n",
        "print(f\"🔢 Features: {len(feature_columns)} pollutant parameters\")\n",
        "print(f\"🏆 Best Model: {best_model_name} (R² = {best_r2_score:.4f})\")\n",
        "print(f\"📈 Performance: MAE = {comparison_df.loc[best_model_name, 'MAE']:.2f}, RMSE = {comparison_df.loc[best_model_name, 'RMSE']:.2f}\")\n",
        "print(f\"🌟 Most Important Feature: {feature_importance.iloc[0]['feature']} ({feature_importance.iloc[0]['importance']:.3f})\")\n",
        "\n",
        "print(\"\\n💡 KEY FINDINGS:\")\n",
        "findings = [\n",
        "    f\"• {best_model_name} performed best with R² = {best_r2_score:.4f}\",\n",
        "    f\"• {feature_importance.iloc[0]['feature'].upper()} is the most predictive feature\",\n",
        "    f\"• All pollutants show moderate correlation with AQI\",\n",
        "    f\"• Model achieved MAE of {comparison_df.loc[best_model_name, 'MAE']:.1f} AQI units\",\n",
        "    \"• Missing values were successfully handled with mean imputation\"\n",
        "]\n",
        "\n",
        "for finding in findings:\n",
        "    print(f\"   {finding}\")\n",
        "\n",
        "print(\"\\n🎯 RECOMMENDATIONS:\")\n",
        "recommendations = [\n",
        "    \"• Include meteorological data for better predictions\",\n",
        "    \"• Implement time series analysis for temporal patterns\",\n",
        "    \"• Consider ensemble methods for improved accuracy\",\n",
        "    \"• Validate with real UCI ML Repository dataset\",\n",
        "    \"• Deploy for real-time air quality monitoring\"\n",
        "]\n",
        "\n",
        "for rec in recommendations:\n",
        "    print(f\"   {rec}\")\n",
        "\n",
        "print(\"\\n✅ AIR QUALITY PREDICTION PROJECT COMPLETED SUCCESSFULLY!\")\n",
        "print(\"📝 All INLIGHN TECH requirements have been implemented!\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}